{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73d6b47",
   "metadata": {},
   "source": [
    "# üìå What is Feature Engineering?\n",
    "\n",
    "- Feature Engineering is the process of **creating, modifying, or selecting variables (features) from raw data** to make machine learning models more effective. Instead of feeding raw data directly into models, we transform it into features that help the model learn better patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a230bc",
   "metadata": {},
   "source": [
    "# ü§î Why Feature Engineering Matters:\n",
    "\n",
    "üöÄ **Improves model performance** ‚Äî more meaningful inputs help predictions. \n",
    "\n",
    "üß© **Highlights patterns** that raw data might hide. \n",
    "\n",
    "üìâ **Reduces noise & overfitting** by removing irrelevant data. \n",
    "\n",
    "üîç **Improves interpretability** ‚Äî easier to explain model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeda9b1",
   "metadata": {},
   "source": [
    "**- What is a Feature?** \n",
    "\n",
    "-   A feature is a **measurable property** or characteristic of the data used as input to an ML model (e.g., age, income, text tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf35e5",
   "metadata": {},
   "source": [
    "# Classification: \n",
    "\n",
    "1. **Feature Transformation** : transforming feature from one to another\n",
    "2. **Feature Construction** : Adding a feature for better performance\n",
    "3. **Feature Selection** : select important feature for our model\n",
    "4. **Feature Extraction** : extract feature according to model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271fe2d8",
   "metadata": {},
   "source": [
    "# 1. üîÑ **Feature Transformation**\n",
    "\n",
    "- It includes:\n",
    "\n",
    "    1. Missing value imputation<br>\n",
    "    2. Handling categorical values<br>\n",
    "    3. Outlier detection<br>\n",
    "    4. Feature scaling:<br>\n",
    "        1. Standardization<br>\n",
    "        2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c6c7a",
   "metadata": {},
   "source": [
    "#### 1. Missing value imputation\n",
    "\n",
    "- There are many methods for missing value.\n",
    "\n",
    "- like get a mean, median or most frequent categorical data etc according to the model, this all are called imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeed469",
   "metadata": {},
   "source": [
    "#### 2. Handling categorical values\n",
    "\n",
    "- We do One hot encoding\n",
    "\n",
    "- In this, category is represented by a separate column with a 1 indicating its presence and 0s for all other categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dccb3",
   "metadata": {},
   "source": [
    "#### 3. Outlier detection\n",
    "\n",
    "- We can find outliers with plotting graph \n",
    "\n",
    "- like boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f869e2a",
   "metadata": {},
   "source": [
    "#### 4. Feature scaling\n",
    "\n",
    "- It is a **preprocessing technique used in machine learning to transform the values of numerical features to a common scale.**\n",
    "\n",
    "- two methods scaling and normalization we show it in detail on tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede5427",
   "metadata": {},
   "source": [
    "# 2. üÜï **Feature Construction**\n",
    "\n",
    "- It is the process of **creating new input features (variables) from existing data** to improve the performance of machine learning models.\n",
    "\n",
    "- Feature construction involves:\n",
    "\n",
    "    - Combining existing features (e.g., creating interaction terms).\n",
    "\n",
    "    - Transforming features (e.g., log, square, or polynomial transformations).\n",
    "\n",
    "    - Creating new features from domain knowledge (e.g., extracting date parts from a timestamp).\n",
    "\n",
    "    - Encoding categorical variables in meaningful ways .\n",
    "\n",
    "\n",
    "**Example**: Feature Construction in a Dataset\n",
    "\n",
    "- Suppose you have a dataset of **house sales** with the following columns:\n",
    "\n",
    "    `bedrooms` (number of bedrooms)\n",
    "\n",
    "    `bathrooms` (number of bathrooms)\n",
    "\n",
    "    `area` (total area in square feet)\n",
    "\n",
    "    `year_built` (year the house was built)\n",
    "\n",
    "\n",
    "- **Original Features**:\n",
    "\n",
    "    `bedrooms`\n",
    "\n",
    "    `bathrooms`\n",
    "\n",
    "    `area`\n",
    "\n",
    "    `year_built`\n",
    "\n",
    "\n",
    "- **Constructed Features**:\n",
    "\n",
    "    - Total rooms: `bedrooms + bathrooms` (captures overall room count).\n",
    "\n",
    "    - Area per room: `area / (bedrooms + bathrooms)` (captures space per room).\n",
    "\n",
    "    - House age: **2026 - year_built** (captures how old the house is).\n",
    "\n",
    "    - Is recent: 1 if (2026 - year_built) <= 10 else 0 (captures if the house is recently built).\n",
    "\n",
    "These new features may help the model better understand the relationship between house characteristics and price, potentially improving predictions ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b455c62",
   "metadata": {},
   "source": [
    "# 3. üßπ **Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223b0f",
   "metadata": {},
   "source": [
    "- **Remove redundant or irrelevant features to reduce model complexity**.\n",
    "\n",
    "**Example**: Feature Selection in a Dataset\n",
    "\n",
    "- Suppose you have a dataset for predicting house prices with the following features:\n",
    "\n",
    "    `bedrooms`\n",
    "\n",
    "    `bathrooms`\n",
    "\n",
    "    `area`\n",
    "\n",
    "    `age`\n",
    "\n",
    "    `garage`\n",
    "\n",
    "    `distance_to_city`\n",
    "\n",
    "    `color (e.g., red, blue, green)`\n",
    "\n",
    "\n",
    "- Feature Selection Process:\n",
    "\n",
    "    - **Irrelevant Feature**: color is unlikely to affect house price, so it can be removed.\n",
    "\n",
    "    - **Redundant Feature**: If garage and area are highly correlated (e.g., all houses with a garage have large area), you might keep only one.\n",
    "\n",
    "    - **Relevant Features**: `bedrooms`, `bathrooms`, `area`, `age`, and `distance_to_city` are likely to be important for predicting price.\n",
    "\n",
    "- After selection, your model uses only the most relevant features: `bedrooms`, `bathrooms`, `area`, `age`, and `distance_to_city`.\n",
    "\n",
    "* **?Why Is Feature Selection Important**?\n",
    "\n",
    "    - Improves model accuracy and generalization.\n",
    "\n",
    "    - Reduces computational cost and training time.\n",
    "\n",
    "    - Helps avoid overfitting by removing noise.\n",
    "\n",
    "    - Makes models simpler and easier to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc195e48",
   "metadata": {},
   "source": [
    "# 4. üîé **Feature Extraction**\n",
    "\n",
    "- Feature extraction is the process of **transforming raw data into a set of new, more informative features** that can be used by machine learning models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
